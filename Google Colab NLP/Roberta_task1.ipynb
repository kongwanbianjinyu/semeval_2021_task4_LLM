{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3501,"status":"ok","timestamp":1639178220671,"user":{"displayName":"jiachen jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17480481963732476825"},"user_tz":300},"id":"XptIh4IoTOl8","outputId":"aea87dd5-3306-4961-b304-04d138dc9d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","import os,sys\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'SemEval2021-Reading-Comprehension-of-Abstract-Meaning-master'\n","GOOGLE_DRIVE_PATH = os.path.join('drive','My Drive',GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","#print(os.listdir(GOOGLE_DRIVE_PATH))\n","sys.path.append(GOOGLE_DRIVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1639178220672,"user":{"displayName":"jiachen jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17480481963732476825"},"user_tz":300},"id":"DiA60WAHTX5e","outputId":"1dbbe980-f680-4e9c-efd5-96cfc2be5d13"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/SemEval2021-Reading-Comprehension-of-Abstract-Meaning-master\n"]}],"source":["%cd $GOOGLE_DRIVE_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4163,"status":"ok","timestamp":1639178224830,"user":{"displayName":"jiachen jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17480481963732476825"},"user_tz":300},"id":"QC_ZANZVTt6B","outputId":"d7119f0b-3a42-4a38-8d5e-5ef45212d539"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-lightning==1.2.3 in /usr/local/lib/python3.7/dist-packages (1.2.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n","Requirement already satisfied: tensorboard\u003e=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (2.7.0)\n","Requirement already satisfied: future\u003e=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (0.18.2)\n","Requirement already satisfied: fsspec[http]\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (2021.11.1)\n","Requirement already satisfied: tqdm\u003e=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (4.62.3)\n","Requirement already satisfied: PyYAML!=5.4.*,\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (6.0)\n","Requirement already satisfied: torch\u003e=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (1.10.0+cu111)\n","Requirement already satisfied: numpy\u003e=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.3) (1.19.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (2.23.0)\n","Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.42.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.4.6)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.35.0)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (57.4.0)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.37.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (3.3.6)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.6.1)\n","Requirement already satisfied: protobuf\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (3.17.3)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py\u003e=0.4-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.15.0)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (4.2.4)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (4.8)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.2.8)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (1.3.0)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (4.8.2)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (3.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (3.10.0.2)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (2021.10.8)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.2.3) (3.1.1)\n","Requirement already satisfied: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow!=4.0.0,\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (1.7.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (21.2.0)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (2.0.8)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (5.2.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (1.2.0)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (4.0.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=0.8.1-\u003epytorch-lightning==1.2.3) (0.13.0)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2018.9)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.1.0)\n"]}],"source":["!pip install pytorch-lightning==1.2.3 transformers datasets"]},{"cell_type":"markdown","metadata":{"id":"gWbr9tYMUnN0"},"source":["# **SemvalDataModule**"]},{"cell_type":"markdown","metadata":{"id":"kUDGUVraZF1j"},"source":["This class load the data from semval Task1 and convert it to tokens. It defines the train_dataloader and val_dataloader function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfnRlIJNUs9d"},"outputs":[],"source":["import pytorch_lightning as pl\n","import datasets\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from typing import Optional, Dict\n","from functools import partial\n","from tqdm import tqdm\n","\n","\n","class SemvalDataModule(pl.LightningDataModule):\n","    def __init__(\n","            self,\n","            model_name_or_path: str = 'google/electra-large-discriminator',\n","            task_name: str = 'DUMA-electra',\n","            max_seq_length: int = 256,\n","            train_batch_size: int = 2,\n","            eval_batch_size: int = 2,\n","    ):\n","        super().__init__()\n","        self.model_name_or_path = model_name_or_path\n","        self.task_name = task_name\n","        self.max_seq_length = max_seq_length\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n","        self.dataset = None\n","\n","        # self.encoded_dataset = None\n","\n","    def setup(self,stage: Optional[str] = None):\n","        preprocessor = partial(self.preprocess, self.tokenizer)\n","        if stage == 'fit':\n","          self.dataset = load_dataset('json', data_files={'train':'data/training_data/Task_1_train.jsonl','dev':'data/training_data/Task_1_dev.jsonl'})\n","\n","          print('Encoding the training datset...')\n","          #print(preprocessor(self.dataset['train'][0]))\n","          self.dataset['train'] = self.dataset['train'].map(preprocessor)\n","          print('Encoding the validation datset...')\n","          self.dataset['dev'] = self.dataset['dev'].map(preprocessor)\n","          print(self.dataset)\n","          #print(self.dataset['dev'][0]['input_ids'])\n","          self.dataset['train'].set_format(type='torch',columns=['input_ids', 'attention_mask', 'label'])\n","          self.dataset['dev'].set_format(type='torch',columns=['input_ids', 'attention_mask', 'label'])\n","          print(self.dataset['dev'][0]['input_ids'])\n","\n","    def train_dataloader(self):  \n","        return DataLoader(self.dataset['train'],\n","                          sampler=RandomSampler(self.dataset['train']),\n","                          batch_size=self.train_batch_size,\n","                          drop_last=True,\n","                          )\n","    def val_dataloader(self):\n","        return DataLoader(self.dataset['dev'],\n","                          sampler=RandomSampler(self.dataset['dev']),\n","                          batch_size=self.eval_batch_size,\n","                          drop_last=True,\n","                          )\n","    @staticmethod\n","    def preprocess(tokenizer, x: Dict)-\u003eDict:\n","        \n","        choices_features = []\n","        option_names = ['option_0','option_1','option_2','option_3','option_4']\n","        \n","        question = x[\"question\"]\n","        article = x[\"article\"]\n","\n","        for option in option_names:\n","\n","            question_option = question.replace(\"@placeholder\", x[option])\n","\n","            inputs = tokenizer(\n","                article,\n","                question_option,\n","                add_special_tokens=True,\n","                max_length=256,\n","                truncation=\"only_first\",\n","                padding='max_length',\n","                return_tensors='pt'\n","            )\n","\n","            choices_features.append(inputs)\n","\n","\n","        label = torch.tensor([x[\"label\"]])\n","\n","        return {\n","            \"label\": label,\n","            \"input_ids\": torch.cat([cf[\"input_ids\"] for cf in choices_features]).reshape(-1),\n","            \"attention_mask\": torch.cat([cf[\"attention_mask\"] for cf in choices_features]).reshape(-1),\n","            #\"token_type_ids\": torch.cat([cf[\"token_type_ids\"] for cf in choices_features]).reshape(-1),\n","        }\n","\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"X1Fb9iakcI78"},"source":["# **DUMAForSemval**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vd2CuONtcF0l"},"outputs":[],"source":["import pytorch_lightning as pl\n","from transformers.modeling_outputs import MultipleChoiceModelOutput\n","from transformers import AutoConfig,AutoModel\n","from transformers import AdamW\n","import torch\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import MultiheadAttention\n","\n","class DUMAForSemval(pl.LightningModule):\n","    def __init__(\n","            self,\n","            pretrained_model: str = 'google/electra-large-discriminator',\n","            learning_rate: float = 1e-4,\n","            gradient_accumulation_steps: int = 32,\n","            num_train_epochs: float = 4.0,\n","            train_batch_size: int = 2,\n","            train_all: bool = False,\n","    ):\n","        super().__init__()\n","        self.config = AutoConfig.from_pretrained(pretrained_model)\n","        self.electra = AutoModel.from_pretrained(pretrained_model,config=self.config)\n","        self.classifier = nn.Linear(self.config.hidden_size, 1)\n","\n","        if not train_all:\n","            for param in self.electra.parameters():\n","                param.requires_grad = False\n","\n","\n","        self.learning_rate = learning_rate\n","        self.gradient_accumulation_steps = gradient_accumulation_steps\n","        self.num_train_epochs = num_train_epochs\n","        self.train_batch_size = train_batch_size\n","\n","    def forward(\n","            self,\n","            input_ids=None, #(batch_size,num_choices,sequence_length:256)\n","            attention_mask=None,\n","            #token_type_ids=None,\n","            labels=None,\n","    ):\n","\n","        input_ids = input_ids.reshape(self.train_batch_size,5,-1)\n","        attention_mask = attention_mask.reshape(self.train_batch_size,5,-1)\n","        #token_type_ids = token_type_ids.reshape(self.train_batch_size,5,-1)\n","\n","        #print(input_ids)\n","        #print(input_ids.shape)\n","\n","        num_choices = input_ids.shape[1] \n","\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1)) #(batch_size*num_choice,sequence_length:256)\n","        #print(input_ids)\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) \n","        #token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) \n","\n","        outputs = self.electra(\n","            input_ids = input_ids,\n","            attention_mask=attention_mask,\n","            #token_type_ids=token_type_ids,\n","        )\n","\n","        last_output = outputs.last_hidden_state #(batch_size, sequence_length:256, hidden_size:256)\n","\n","        pooled_output = torch.mean(last_output, dim=1)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        loss = None\n","        loss_fct = CrossEntropyLoss()\n","        loss = loss_fct(reshaped_logits, labels)\n","\n","        \n","        # output = (reshaped_logits,) + outputs[2:]\n","        # return ((loss,) + output) if loss is not None else output\n","\n","        return MultipleChoiceModelOutput(\n","            loss=loss,\n","            logits=reshaped_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        # input training batch, calling DUMA forward() function\n","        # return loss\n","        outputs = self(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            #token_type_ids=batch['token_type_ids'],\n","            labels=batch['label'],\n","        )\n","        labels_hat = torch.argmax(outputs.logits, dim=1)\n","        correct_count = torch.sum(batch['label'] == labels_hat)\n","        loss = outputs.loss\n","        self.log('train_loss', loss)\n","        self.log('train_acc', correct_count.float() / len(batch['label']))\n","        #print('train_acc',correct_count.float() / len(batch['label']))\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","      # input validation batch, calling DUMA forward() function\n","      # return loss\n","        outputs = self(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            #token_type_ids=batch['token_type_ids'],\n","            labels=batch['label'],\n","        )\n","        labels_hat = torch.argmax(outputs.logits, dim=1)\n","        correct_count = torch.sum(batch['label'] == labels_hat)\n","        loss = outputs.loss\n","\n","        return {\n","            \"val_loss\": loss,\n","            \"correct_count\": correct_count,\n","            \"batch_size\": len(batch['label'])\n","        }\n","    def validation_epoch_end(self, outputs) -\u003e None:\n","        val_acc = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n","        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n","        self.log('val_acc', val_acc)\n","        self.log('val_loss', val_loss)\n","        print('val_loss', val_loss)\n","        print('val_acc', val_acc)\n","    \n","    def configure_optimizers(self):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        return AdamW(optimizer_grouped_parameters, lr=self.learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"zAVsx1egInKQ"},"source":["# **Trainer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xj2NtD9AImdq"},"outputs":[],"source":["\n","# For ELECTRA + DUMA\n","model_name = 'roberta-large'\n","model = DUMAForSemval(\n","        pretrained_model= model_name,\n","        learning_rate=1e-4,\n","        num_train_epochs=1.0,\n","        train_batch_size=2,\n","        train_all=False,\n","    )\n","data = SemvalDataModule(\n","        model_name_or_path= model_name,\n","        train_batch_size=2,\n","        eval_batch_size=2,\n","        max_seq_length=256,\n","    )\n","trainer = pl.Trainer(\n","        gpus= 1 ,\n","        #auto_scale_batch_size='power',\n","        #auto_lr_find=True,\n","        max_epochs=1,\n","        val_check_interval=0.2,\n","    )\n","trainer.fit(model, data)\n","trainer.save_checkpoint('roberta-large_sem/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tdawONVf4YaU"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7865c600de5490d96122715891b3a23","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/668 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e58d81da4544fa38bd9073b3932efeb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.25G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a1761c764974f32bf957c717408709c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/27.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62dab76833604649b7c525c3047c47b9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35a2fc217a5743a5b78e4f45f15fdd89","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","Using custom data configuration default-7ce9c725959407f3\n","Reusing dataset json (/root/.cache/huggingface/datasets/json/default-7ce9c725959407f3/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd35ff1acbb4471cb2e80f7092ea3d2a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Encoding the training datset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"758de5a34de94f878522fb6b081836ff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3227 [00:00\u003c?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Encoding the validation datset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c45a5628c8124c25b31cecb94a85d174","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/837 [00:00\u003c?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['article', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'option_4', 'label', 'input_ids', 'attention_mask'],\n","        num_rows: 3227\n","    })\n","    dev: Dataset({\n","        features: ['article', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'option_4', 'label', 'input_ids', 'attention_mask'],\n","        num_rows: 837\n","    })\n","})\n","tensor([ 101, 2260, 2238,  ...,    0,    0,    0])\n"]},{"name":"stderr","output_type":"stream","text":["\n","  | Name       | Type         | Params\n","--------------------------------------------\n","0 | electra    | ElectraModel | 334 M \n","1 | classifier | Linear       | 1.0 K \n","--------------------------------------------\n","1.0 K     Trainable params\n","334 M     Non-trainable params\n","334 M     Total params\n","1,336.373 Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee074138157648dca8379e2e343b83e5","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_loss tensor(1.6082, device='cuda:0')\n","val_acc tensor(0.5000, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85fecca40a054d6d8d751c8e925ffb94","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c9434144f344a1f9a0642c32b13c4dc","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_loss tensor(1.5820, device='cuda:0')\n","val_acc tensor(0.8660, device='cuda:0')\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"847bc141c8de4aa2ad6ac795e4c8bcc8","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_loss tensor(1.5557, device='cuda:0')\n","val_acc tensor(0.8684, device='cuda:0')\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56318bc3f5f84ec3949d87ec2848a8b7","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_loss tensor(1.5312, device='cuda:0')\n","val_acc tensor(0.8696, device='cuda:0')\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"789acdf0eebc467fb8faaf20e7faa7c1","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_loss tensor(1.5082, device='cuda:0')\n","val_acc tensor(0.8684, device='cuda:0')\n"]},{"data":{"text/plain":["1"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["\n","model_name = 'google/electra-large-discriminator'\n","model = DUMAForSemval(\n","        pretrained_model= model_name,\n","        learning_rate=1e-4,\n","        num_train_epochs=4.0,\n","        train_batch_size=2,\n","        train_all=False,\n","    )\n","data = SemvalDataModule(\n","        model_name_or_path= model_name,\n","        train_batch_size=2,\n","        eval_batch_size=2,\n","        max_seq_length=256,\n","    )\n","trainer = pl.Trainer(\n","        gpus= 1 ,\n","        #auto_scale_batch_size='power',\n","        #auto_lr_find=True,\n","        max_epochs=4,\n","        #val_check_interval=0.2,\n","    )\n","trainer.fit(model, data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9DvWbXEn2QYE"},"outputs":[],"source":["\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Roberta_task1.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}